{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASAP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize constants. Documents with num of words in sentence > MAX_WORD_PER_SENTENCE or num of sentences in doc > MAX_SENTENCE_PER_DOC will be discarded. Words that appear in data less than MIN_FREQ_WORD_NUM wiil be changed to '__UNK_WORD__' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_PER_SENTENCE = 50\n",
    "MAX_SENTENCE_PER_DOC = 50\n",
    "MIN_FREQ_WORD_NUM = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('processed_dataset.csv')\n",
    "df.drop(df[df['essay_set']==9].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1783, 1800, 1726, 1770, 1805, 1800, 1569, 723]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(df[df['essay_set']==i]) for i in range(1,9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling all scores to 0-60 scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,9):\n",
    "    temp = df.loc[df[\"essay_set\"]==i,\"score\"]\n",
    "    df.loc[df[\"essay_set\"]==i,\"score\"] = 60*(temp-np.min(temp))/(np.max(temp)-np.min(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing dataset to labels and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df['essay']\n",
    "labels=df['score']\n",
    "data_np = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(docs):\n",
    "    \"\"\"\n",
    "    Tokenize texts into sentences using nltk.\n",
    "    \n",
    "    docs: ndarray with texts, shape=(num_texts,)\n",
    "    \n",
    "    returns: documents - list with tokenized texts \n",
    "    \"\"\"\n",
    "    sent_tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n",
    "    documents = []\n",
    "    for doc in docs:\n",
    "        sentences = sent_tokenizer.tokenize(doc)\n",
    "        documents.append(sentences)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z,!?'`]\", \" \", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_words(documents):\n",
    "    \"\"\"\n",
    "    Tokenize sentence to words/removing text due to predefined constants.\n",
    "    \n",
    "    documents: list with tokenized texts (from read function)\n",
    "    \n",
    "    returns: new_documents - list with texts tokenized to words\n",
    "             counter - Counter with frequencies of words\n",
    "    \"\"\"\n",
    "    new_documents = []\n",
    "    counter = Counter()\n",
    "    drop=[]\n",
    "    for i, doc in enumerate(documents):\n",
    "        document = []\n",
    "        \n",
    "        discard = False\n",
    "        for sentence in doc:\n",
    "            n_sentence = []\n",
    "            words = clean_str(sentence).split(\" \")\n",
    "            # if any sentence's length is over  MAX_WORD_PER_SENTENCE,\n",
    "            # discard the whole document for simplicity\n",
    "            if len(words) > MAX_WORD_PER_SENTENCE:\n",
    "                discard = True\n",
    "                break\n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if word:\n",
    "                    n_sentence.append(word)\n",
    "                    counter[word] += 1\n",
    "            if n_sentence:\n",
    "                document.append(n_sentence)\n",
    "        # only accept document that has more than one sentence and less than MAX_SENTENCE_PER_DOC,\n",
    "        # again, for simplicity's sake\n",
    "        if 1 < len(document) <= MAX_SENTENCE_PER_DOC and not discard:\n",
    "            new_documents.append(document)\n",
    "        else:\n",
    "            drop.append(i)\n",
    "    labels.drop(drop,inplace=True)\n",
    "    return new_documents, counter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(n):\n",
    "        num = 0\n",
    "        for k, v in counter.items():\n",
    "            if v >= n:\n",
    "                num += 1\n",
    "        return num\n",
    "#print('len of vocabulary：%s' % len(counter))\n",
    "#print('number of frequency more than %d：%s' % (5, freq(5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(docs_processed,counter):\n",
    "    \"\"\"\n",
    "    Replacing rare words with '__UNK_WORD__' token.\n",
    "    \n",
    "    docs_preprocessed: list with texts tokenized to words (from split_to_word function)\n",
    "    counter - Counter with frequencies of words (from split_to_word function)\n",
    "    \n",
    "    \"\"\"\n",
    "    for doc_id in range(len(docs_processed)):\n",
    "        for sen_id in range(len(docs_processed[doc_id])):\n",
    "            for word_id in range(len(docs_processed[doc_id][sen_id])):\n",
    "                word = docs_processed[doc_id][sen_id][word_id]\n",
    "                if counter[word] < 10:\n",
    "                    docs_processed[doc_id][sen_id][word_id] = '__UNK_WORD__'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(vocab, vocab_file):\n",
    "    \"\"\"\n",
    "    Write vocabulary to file.\n",
    "    \n",
    "    vocab: dict (word : word_code)\n",
    "    vocab_file: string, filename\n",
    "    \"\"\"\n",
    "    with open(vocab_file, 'w') as f:\n",
    "        for word, index in vocab.items():\n",
    "            f.write(word+' '+str(index)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of documents: 10447\n",
      "max num of document sentences：50\n",
      "min num of document sentences：2\n",
      "avg num of document sentences：12.821479850674836\n",
      "max num of sentence words：50\n",
      "min num of sentence words：1\n",
      "avg num of sentence words：17.159191017275617\n",
      "vocab len：6068\n"
     ]
    }
   ],
   "source": [
    "def pre_process(docs):\n",
    "    \"\"\"\n",
    "    Preprocess dataset/save vocab.\n",
    "    \n",
    "    docs: ndarray with texts, shape=(num_texts,)\n",
    "    \n",
    "    returns: data_processed - preprocessed dataset\n",
    "             vocab - vocabulary of all word in preprocessed dataset\n",
    "    \"\"\"\n",
    "    data = read(docs)\n",
    "    data_processed, counter = split_to_words(data)\n",
    "    process_doc(data_processed, counter)\n",
    "    word_index = 0\n",
    "    vocab = {}\n",
    "    for doc in data_processed:\n",
    "        for sen in doc:\n",
    "            for word in sen:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = word_index\n",
    "                    word_index += 1\n",
    "\n",
    "    doc_len = []\n",
    "    sentence_len = []\n",
    "    for doc in data_processed:\n",
    "        doc_len.append(len(doc))\n",
    "        for sen in doc:\n",
    "            sentence_len.append(len(sen))\n",
    "    print('total number of documents: %s' % (len(data_processed)))\n",
    "    print('max num of document sentences：%s' % max(doc_len))\n",
    "    print('min num of document sentences：%s' % min(doc_len))\n",
    "    print('avg num of document sentences：%s' % (float(sum(doc_len))/len(doc_len)))\n",
    "\n",
    "    print('max num of sentence words：%s' % max(sentence_len))\n",
    "    print('min num of sentence words：%s' % min(sentence_len))\n",
    "    print('avg num of sentence words：%s' % (float(sum(sentence_len))/len(sentence_len)))\n",
    "    \n",
    "    print('vocab len：%s' % len(vocab))\n",
    "    \n",
    "    write_vocab(vocab, 'vocab.txt')\n",
    "    return data_processed, vocab\n",
    "\n",
    "all_data, vocab = pre_process(data_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding every word with unique code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_doc(lst, vocab):\n",
    "    \"\"\"\n",
    "    Encode every word in dataset with its code & pad all texts with zeros.\n",
    "    \n",
    "    lst: preprocessed dataset\n",
    "    vocab - vocabulary of all word in preprocessed dataset\n",
    "    \n",
    "    returns: ndarray with encoded & padded data, shape=(num_docs, sentence_max_num, sentence_max_len) \n",
    "    \"\"\"\n",
    "    sentence_max_len = max([max([len(sen) for sen in doc]) for doc in lst])\n",
    "    sentence_max_num = max(map(len, lst))\n",
    "    result = np.zeros([len(lst), sentence_max_num, sentence_max_len], dtype=np.int32)\n",
    "    for i, row in enumerate(lst):\n",
    "        for j, col in enumerate(row):\n",
    "            for k, val in enumerate(col):\n",
    "                result[i][j][k] = vocab[val]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded=encode_doc(all_data,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_labels = np.array(labels)\n",
    "np_labels = (np_labels-np.min(np_labels))/(np.max(np_labels)-np.min(np_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded, np_labels, test_size=0.1, random_state=42)\n",
    "np.save('x_train',X_train)\n",
    "np.save('y_train',y_train)\n",
    "np.save('x_test',X_test)\n",
    "np.save('y_test',y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding hyperparameters to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../configs/config.json\", \"r\") as jsonFile:\n",
    "    config = json.load(jsonFile)\n",
    "\n",
    "config[\"max_sent\"] = MAX_SENTENCE_PER_DOC\n",
    "config[\"max_word\"] = MAX_WORD_PER_SENTENCE\n",
    "config[\"vocab_size\"] = len(vocab)\n",
    "config[\"min_rating\"] = 0\n",
    "config[\"max_rating\"] = 60\n",
    "\n",
    "\n",
    "with open(\"../configs/config.json\", \"w\") as jsonFile:\n",
    "    json.dump(config, jsonFile, indent=4, separators=(',', ': '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
