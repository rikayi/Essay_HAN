{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_PER_SENTENCE = 40\n",
    "MAX_SENTENCE_PER_DOC = 40\n",
    "MIN_FREQ_WORD_NUM = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('processed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1783, 1800, 1726, 1770, 1805, 1800, 1569, 723]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(df[df['essay_set']==i]) for i in range(1,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=df[df['essay_set']==1]\n",
    "test=test.drop(['essay_id','essay_set'],axis=1)\n",
    "data=test['essay']\n",
    "\n",
    "labels=test['score']\n",
    "labels_np=labels.values\n",
    "np.save('labels1.npy',labels_np)\n",
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np=data.values\n",
    "np.save('data1.npy',data_np)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "documents = []\n",
    "for i,string in enumerate(data_np):\n",
    "    string = re.sub(r\"['`]\", \"\", string)\n",
    "    string = re.sub(r\"[^A-Za-z(),!?.]\", \" \", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    string = string.strip().lower()\n",
    "    documents.append(np.array(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(docs):\n",
    "    sent_tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n",
    "    documents = []\n",
    "    for doc in docs:\n",
    "        sentences = sent_tokenizer.tokenize(doc)\n",
    "        documents.append(sentences)\n",
    "    return documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_words(documents):\n",
    "    new_documents = []\n",
    "    counter = Counter()\n",
    "    drop=[]\n",
    "    for i, doc in enumerate(documents):\n",
    "        document = []\n",
    "        \n",
    "        discard = False\n",
    "        for sentence in doc:\n",
    "            n_sentence = []\n",
    "            words = clean_str(sentence).split(\" \")\n",
    "            # if any sentence's length is over  MAX_WORD_PER_SENTENCE,\n",
    "            # discard the whole document for simplicity\n",
    "            if len(words) > MAX_WORD_PER_SENTENCE:\n",
    "                discard = True\n",
    "                break\n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if word:\n",
    "                    n_sentence.append(word)\n",
    "                    counter[word] += 1\n",
    "            if n_sentence:\n",
    "                document.append(n_sentence)\n",
    "        # only accept document that has more than one sentence and less than MAX_SENTENCE_PER_DOC,\n",
    "        # again, for simplicity's sake\n",
    "        if 1 < len(document) <= MAX_SENTENCE_PER_DOC and not discard:\n",
    "            new_documents.append(document)\n",
    "        else:\n",
    "            drop.append(i)\n",
    "    labels.drop(drop,inplace=True)\n",
    "    return new_documents, counter\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z,!?'`]\", \" \", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(n):\n",
    "        num = 0\n",
    "        for k, v in counter.items():\n",
    "            if v >= n:\n",
    "                num += 1\n",
    "        return num\n",
    "#print('number of vocabulary：%s' % len(counter))\n",
    "#print('number of frequency more than %d：%s' % (5, freq(5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(docs_processed,counter):\n",
    "        for doc_id in range(len(docs_processed)):\n",
    "            for sen_id in range(len(docs_processed[doc_id])):\n",
    "                for word_id in range(len(docs_processed[doc_id][sen_id])):\n",
    "                    word = docs_processed[doc_id][sen_id][word_id]\n",
    "                    if counter[word] < 10:\n",
    "                        docs_processed[doc_id][sen_id][word_id] = '__UNK_WORD__'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_doc(pos_docs, vocab, filename):\n",
    "    docs = [(1, doc) for doc in pos_docs]\n",
    "    len_to_data = {}\n",
    "    for doc in docs:\n",
    "        doc_len = len(doc[1])\n",
    "        if doc_len in len_to_data:\n",
    "            len_to_data[doc_len].append(doc)\n",
    "        else:\n",
    "            len_to_data[doc_len] = [doc]\n",
    "    for value in len_to_data.values():\n",
    "        random.shuffle(value)\n",
    "    keys = list(len_to_data.keys())\n",
    "    sorted_docs = []\n",
    "    for key in sorted(keys):\n",
    "        sorted_docs.extend(len_to_data[key])\n",
    "    with open(filename, 'w') as f:\n",
    "        for content in sorted_docs:\n",
    "            line = '%d:' % content[0]\n",
    "            for sentence in content[1]:\n",
    "                sentence = [str(vocab[word]) for word in sentence]\n",
    "                line += ','.join(sentence) + '#'\n",
    "            f.write(line[:-1]+'\\n')\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def write_vocab(vocab, vocab_file):\n",
    "    with open(vocab_file, 'w') as f:\n",
    "        for word, index in vocab.items():\n",
    "            f.write(word+' '+str(index)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of documents: 1056, pos: 1056\n",
      "max num of document sentences：40\n",
      "min num of document sentences：2\n",
      "avg num of document sentences：24.107954545454547\n",
      "max num of sentence words：40\n",
      "min num of sentence words：1\n",
      "avg num of sentence words：15.265260428941787\n"
     ]
    }
   ],
   "source": [
    "def pre_process(docs):\n",
    "    pos = read(docs)\n",
    "    \n",
    "    pos_processed, counter = split_to_words(pos)\n",
    "    process_doc(pos_processed, counter)\n",
    "    word_index = 1\n",
    "    vocab = {}\n",
    "    for doc in pos_processed:\n",
    "        for sen in doc:\n",
    "            for word in sen:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = word_index\n",
    "                    word_index += 1\n",
    "\n",
    "    all_docs = pos_processed\n",
    "    doc_len = []\n",
    "    sentence_len = []\n",
    "    for doc in all_docs:\n",
    "        doc_len.append(len(doc))\n",
    "        for sen in doc:\n",
    "            sentence_len.append(len(sen))\n",
    "    print('total number of documents: %s, pos: %s' %\n",
    "          (len(all_docs), len(pos_processed)))\n",
    "    print('max num of document sentences：%s' % max(doc_len))\n",
    "    print('min num of document sentences：%s' % min(doc_len))\n",
    "    print('avg num of document sentences：%s' % (float(sum(doc_len))/len(doc_len)))\n",
    "\n",
    "    print('max num of sentence words：%s' % max(sentence_len))\n",
    "    print('min num of sentence words：%s' % min(sentence_len))\n",
    "    print('avg num of sentence words：%s' % (float(sum(sentence_len))/len(sentence_len)))\n",
    "    \n",
    "    write_doc(pos_processed, vocab, 'data.dat')\n",
    "    write_vocab(vocab, 'vocab.txt')\n",
    "    return all_docs, vocab\n",
    "\n",
    "all_data, vocab = pre_process(data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_doc(lst, vocab):\n",
    "    sentence_max_len = max([max([len(sen) for sen in doc]) for doc in lst])\n",
    "    sentence_max_num = max(map(len, lst))\n",
    "    result = np.zeros([len(lst), sentence_max_num, sentence_max_len], dtype=np.int32)\n",
    "    for i, row in enumerate(lst):\n",
    "        for j, col in enumerate(row):\n",
    "            for k, val in enumerate(col):\n",
    "                result[i][j][k] = vocab[val]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded=encode_doc(all_data,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2300088778409091"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(encoded)/(1056*40*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_labels = np.array(labels)\n",
    "np_labels /= np.max(np_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded, np_labels, test_size=0.1, random_state=42)\n",
    "np.save('x_train1',X_train)\n",
    "np.save('y_train1',y_train)\n",
    "np.save('x_test1',X_test)\n",
    "np.save('y_test1',y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
